{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer-learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xry9F4ar1jzP",
        "outputId": "99ed1037-b22f-48fa-8726-ebc588372ab5"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JFJ5FOYZavL"
      },
      "source": [
        "dataset link:https://drive.google.com/drive/folders/1xKucweYB8pM5fRFpUy9EYkm1oqSVIRy_?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9BoU7K90ugY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c285ad-646b-4d84-d708-9e3d859ba530"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "train_dir = \"/content/drive/MyDrive/gender/Training/\"\r\n",
        "test_dir = \"/content/drive/MyDrive/gender/Validation/\"\r\n",
        "\r\n",
        "\r\n",
        "# Using imagedatagenerator to access the files and image augmentation.\r\n",
        "\r\n",
        "# You can change the setting of imageDataGenerator for other augmentation methods.\r\n",
        "\r\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\r\n",
        "test_datagen = ImageDataGenerator(rescale=1/255)\r\n",
        "\r\n",
        "\r\n",
        "training_generator = train_datagen.flow_from_directory(train_dir,\r\n",
        "                                                      target_size=(224, 224),\r\n",
        "                                                      class_mode=\"binary\")\r\n",
        "\r\n",
        "\r\n",
        "test_generator = test_datagen.flow_from_directory(test_dir,\r\n",
        "                                                       target_size=(224, 224),\r\n",
        "                                                       class_mode=\"binary\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 22667 images belonging to 2 classes.\n",
            "Found 5472 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s589jIyS06gl"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\r\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\r\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\r\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "import keras\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAB4YKGY09u2"
      },
      "source": [
        "target_size = (224, 224)\r\n",
        "batch_size = 128\r\n",
        "mode = 'binary'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj6KpGu71A5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8e1768-b23d-41a8-cb7b-398a8354fcb2"
      },
      "source": [
        "vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGM2YcPu1Bfw"
      },
      "source": [
        "vgg.trainable = False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9chCd_lG1BqO"
      },
      "source": [
        "model_with_tuning = 'transfer_model.h5'\r\n",
        "checkpoint_callback = ModelCheckpoint(model_with_tuning,\r\n",
        "                                     monitor='val_accuracy',\r\n",
        "                                     save_best_only=True,\r\n",
        "                                     verbose=1)\r\n",
        "earlystop = EarlyStopping(monitor='val_accuracy',patience=5,verbose=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S15yV5Y51JR1"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(vgg)\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(256, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjFiL-sA1JCU"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\r\n",
        "             metrics=['accuracy'],\r\n",
        "             optimizer='adam')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBhqbdFm1Nxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "110e983f-5ea3-4fc3-8d8a-a897088a6804"
      },
      "source": [
        "history_without_tuning = model.fit(training_generator,\r\n",
        "                   steps_per_epoch=22667  // batch_size,\r\n",
        "                   epochs=10,\r\n",
        "                   validation_data=test_generator,\r\n",
        "                   validation_steps=5472 // batch_size,\r\n",
        "                   verbose=1,\r\n",
        "                   callbacks = [checkpoint_callback,earlystop])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "177/177 [==============================] - 2565s 14s/step - loss: 0.7228 - accuracy: 0.7874 - val_loss: 0.2150 - val_accuracy: 0.9144\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.91443, saving model to transfer_model.h5\n",
            "Epoch 2/10\n",
            "177/177 [==============================] - 2043s 12s/step - loss: 0.1820 - accuracy: 0.9315 - val_loss: 0.1603 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.91443 to 0.93452, saving model to transfer_model.h5\n",
            "Epoch 3/10\n",
            "177/177 [==============================] - 1403s 8s/step - loss: 0.1509 - accuracy: 0.9462 - val_loss: 0.1724 - val_accuracy: 0.9390\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.93452 to 0.93899, saving model to transfer_model.h5\n",
            "Epoch 4/10\n",
            "177/177 [==============================] - 1177s 7s/step - loss: 0.1521 - accuracy: 0.9440 - val_loss: 0.2075 - val_accuracy: 0.9182\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.93899\n",
            "Epoch 5/10\n",
            "177/177 [==============================] - 790s 4s/step - loss: 0.1621 - accuracy: 0.9382 - val_loss: 0.1247 - val_accuracy: 0.9501\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.93899 to 0.95015, saving model to transfer_model.h5\n",
            "Epoch 6/10\n",
            "177/177 [==============================] - 649s 4s/step - loss: 0.1305 - accuracy: 0.9525 - val_loss: 0.1454 - val_accuracy: 0.9427\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.95015\n",
            "Epoch 7/10\n",
            "177/177 [==============================] - 437s 2s/step - loss: 0.1199 - accuracy: 0.9571 - val_loss: 0.1357 - val_accuracy: 0.9546\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.95015 to 0.95461, saving model to transfer_model.h5\n",
            "Epoch 8/10\n",
            "177/177 [==============================] - 379s 2s/step - loss: 0.1205 - accuracy: 0.9590 - val_loss: 0.1576 - val_accuracy: 0.9353\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.95461\n",
            "Epoch 9/10\n",
            "177/177 [==============================] - 265s 2s/step - loss: 0.1129 - accuracy: 0.9572 - val_loss: 0.2128 - val_accuracy: 0.9271\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.95461\n",
            "Epoch 10/10\n",
            "177/177 [==============================] - 214s 1s/step - loss: 0.1446 - accuracy: 0.9477 - val_loss: 0.1332 - val_accuracy: 0.9479\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.95461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBI9b6011OLd"
      },
      "source": [
        "model.save('/content/drive/MyDrive/transfer_model.h5')\r\n",
        "#model = keras.models.load_model('/content/drive/MyDrive/transfer_model_model.h5')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFXhluT116dk",
        "outputId": "1da95d26-995d-4a33-b600-77845bf2b885"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "test = \"/content/drive/MyDrive/pic/\"\r\n",
        "\r\n",
        "\r\n",
        "test_data = ImageDataGenerator(rescale=1/255)\r\n",
        "\r\n",
        "\r\n",
        "test_generator = test_data.flow_from_directory(test,\r\n",
        "                                              target_size=(224, 224),\r\n",
        "                                              class_mode=\"binary\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 20 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcvQqfDj4RGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97334fb6-fd29-4ccf-d334-c5748a5d02e3"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator)\r\n",
        "print('test acc:', test_acc)\r\n",
        "print('test_loss:',test_loss)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step - loss: 0.0034 - accuracy: 1.0000\n",
            "test acc: 1.0\n",
            "test_loss: 0.003373892279341817\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}